{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://raw.githubusercontent.com/archivesunleashed/archivesunleashed.org/master/themes/hugo-material-docs/static/images/cropped-logo.png\" height=\"200px\" width=\"500px\">\n",
    "\n",
    "# Welcome\n",
    "\n",
    "Welcome to the Archives Unleashed Cloud Visualization Demo in Jupyter Notebook for your collection. This demonstration takes the main derivatives from the Cloud and uses Python to analyze and produce information about your collection.\n",
    "\n",
    "This product is in beta, so if you encounter any issues, please post an [issue in our Github repository](https://github.com/archivesunleashed/auk/issues) to let us know about any bugs you encountered or features you would like to see included.\n",
    "\n",
    "If you have some basic Python coding experience, you can change the code we provided to suit your own needs.\n",
    "\n",
    "Unfortunately, we cannot support code that you produced yourself. We recommend that you use `File > Make a Copy` first before changing the code in the repository. That way, you can always return to the basic visualizations we have offered here. Of course, you can also just re-download the Jupyter Notebook file from your Archives Unleashed Cloud account.\n",
    "\n",
    "### How Jupyter Notebooks Work:\n",
    "\n",
    "If you have no previous experience of Jupyter Notebooks, the most important thing to understand is that that <Shift><Enter/Return> will run the python code inside a window and output it to the site.\n",
    "    \n",
    "The window titled `# RUN THIS FIRST` should be the first place you go. This will import all the libraries and set basic variables (e.g. where your derivative files are located) for the notebook. After that, everything else should be able to run on its own.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS FIRST\n",
    "\n",
    "# This Window will set up all the necessary libraries and dependencies\n",
    "# for your Collection.\n",
    "coll_id = \"4656\"\n",
    "auk_fp = \"data/\"\n",
    "auk_full_text = auk_fp + coll_id + \"-fulltext.txt\"\n",
    "auk_gephi = auk_fp + coll_id + \"-gephi.gexf\"\n",
    "auk_graphml = auk_fp + coll_id + \"-gephi.grapml\"\n",
    "auk_domains = auk_fp + coll_id + \"-fullurls.txt\"\n",
    "auk_filtered_text = auk_fp + coll_id + \"-filtered_text.zip\"\n",
    "\n",
    "# The following script will attempt to install the necessary dependencies\n",
    "# for the visualisations. You may prefer to install these on your\n",
    "# own in the command line.\n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "try:  # a library for manipulating column data.\n",
    "    import pandas as pd\n",
    "except ImportError:\n",
    "    !{sys.executable} -m pip install pandas  \n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt # a library for Plotting\n",
    "except ImportError:\n",
    "    !{sys.executable} -m pip install matplotlib\n",
    "\n",
    "try:\n",
    "    import numpy as np # a library for complex mathematics\n",
    "except ImportError:\n",
    "    !{sys.executable} -m pip install numpy\n",
    "    \n",
    "try:\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.draw.dispersion import dispersion_plot as dp\n",
    "except ImportError:\n",
    "    !{sys.executable} -m pip install nltk\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis\n",
    "\n",
    "The following set of functions use the [Natural Language Toolkit](https://www.nltk.org) Python library to search for the top most used words in the collection, as well as facilitate breaking it down by name or domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can change the value of `top` to get more results. \n",
    "top = 30\n",
    "\n",
    "def clean_domain(s):\n",
    "    stop_words = [\"com\", \"org\", \"net\", \"edu\"]\n",
    "    ret = \"\"\n",
    "    dom = s.split(\".\")\n",
    "    if len(dom) <3:\n",
    "        ret = dom[0]\n",
    "    elif dom[-2] in stop_words:\n",
    "        ret = dom[-3]\n",
    "    else:\n",
    "        ret = dom[1]\n",
    "    return ret\n",
    "\n",
    "def get_textfile (minlen=3) :\n",
    "    tokens = []\n",
    "    with open (auk_full_text) as fin:\n",
    "        for line in fin:\n",
    "            tokens += word_tokenize(str(line).split(\",\")[3])\n",
    "    tokens = [x for x in tokens if len(x) > minlen]\n",
    "    return tokens\n",
    "\n",
    "def get_text_domains(minlen=3):\n",
    "    tokens = []\n",
    "    with open (auk_full_text) as fin:\n",
    "        for line in fin:\n",
    "            split_line = str(line).split(',')\n",
    "            tokens.append((clean_domain(split_line[1]), Counter([x for x in word_tokenize(str(split_line[3])) if len(x) > minlen])))\n",
    "    return tokens\n",
    "\n",
    "def get_text_years(minlen=3):\n",
    "    tokens = []\n",
    "    with open (auk_full_text) as fin:\n",
    "        for line in fin:\n",
    "            split_line = str(line).split(',')\n",
    "            tokens.append((split_line[0][1:5], Counter([x for x in word_tokenize(str(split_line[3])) if len(x) > minlen])))\n",
    "    return tokens\n",
    "\n",
    "def year(minlen=3):\n",
    "    return get_text_years(minlen)\n",
    "\n",
    "def domain(minlen=3):\n",
    "    return get_text_domains(minlen)\n",
    "\n",
    "def get_top_tokens(total=20, minlen=3):\n",
    "    return [(key, value) for key, value in Counter(get_textfile(minlen)).most_common(total)]\n",
    "\n",
    "def get_top_tokens_by(fun, total=20, minlen=3):\n",
    "    sep = dict()\n",
    "    tokens = fun(minlen)\n",
    "    sep = {k[0]: Counter() for k in tokens}\n",
    "    for key, value in tokens:\n",
    "        sep[key] += value\n",
    "    ret = [(key, val.most_common(total)) for key, val in sep.items()]\n",
    "    return (ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have saved the above functions, you can now use them in a variety of ways. \n",
    "\n",
    "### Text by Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the set of available years in the collection \n",
    "set([x[0] for x in get_text_years()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create separate lists with text files from individual years in this collection. The example below selects all items from the year 2016. You may need to change it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_results = [t[1] for t in get_text_years() if t[0] == \"2016\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first ten results from the year specified above\n",
    "year_results[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may now want to export this file so you can work with it. \n",
    "# this will appear in the directory that this notebook is in\n",
    "# you may want to change the output path\n",
    "\n",
    "with open(\"results-2016.txt\", \"w\") as output_file:\n",
    "    for value in year_results:\n",
    "        output_file.write(str(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text by Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the set of available domains in the collection \n",
    "set([x[0] for x in get_text_domains()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract only the given domain to a file and see how many results there are\n",
    "\n",
    "domain_results = [t[1] for t in get_text_domains() if t[0] == \"nanaimodailynews\"]\n",
    "len(domain_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first five results from the year specified above\n",
    "domain_results[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may now want to export this file so you can work with the text of one domain. \n",
    "# this will appear in the directory that this notebook is in\n",
    "# you may want to change the output path\n",
    "\n",
    "with open(\"results-domain.txt\", \"w\") as output_file:\n",
    "    for value in domain_results:\n",
    "        output_file.write(str(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Collection Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of the top words in the collection\n",
    "# (regardless of year).\n",
    "get_top_tokens(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of the top tokens, separated by year.\n",
    "get_top_tokens_by(year, top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of top tokens, separated by domain.\n",
    "get_top_tokens_by(domain, top, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dispersion plot, showing where the list of words appear\n",
    "# in the text.\n",
    "text = get_textfile()\n",
    "dp(text, [\"he\", \"she\"]) # uses the nltk dispersion plot library (dp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography\n",
    "\n",
    "Bird, Steven, Edward Loper and Ewan Klein (2009), *Natural Language       Processing with Python*. Oâ€™Reilly Media Inc.\n",
    "\n",
    "Archives Unleashed Project. (2018). Archives Unleashed Toolkit (Version 0.17.0). Apache License, Version 2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
